{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab2dd43-09ae-41b0-bce6-7a3b6f749c53",
   "metadata": {},
   "source": [
    "# Word2Vec training and Evaluation\n",
    "\n",
    "The objective of this assignment is to learn distributed word representations that capture syntactic and semantic relation between them, using Skip-gram with negative sampling, published in [_Mikolov et al,_](https://browse.arxiv.org/pdf/1310.4546.pdf). The paper mentioned above reviews approaches used for the task at hand in the past  like **Continous bag of words**, **Skip-gram** and challenges associated with them. Where **Skip-gram** made the process of learning word representaions efficient, _Mikolov et. al._ suggested following extentions to Skip-gram model which further improved it around 2X-10X:\n",
    "\n",
    "- Hierarchical softmax\n",
    "- Negative sampling\n",
    "- Noice contrastive estimation\n",
    "\n",
    "Here in this notebook I have worked on **Skip-gram with negative sampling**. The project has following folder structure:\n",
    "```\n",
    "Word2Vec\n",
    "├── Artifacts\n",
    "│   ├── metadata\n",
    "│   └── model\n",
    "├── Data\n",
    "│   ├── ds1.txt\n",
    "│   └── ds2.txt\n",
    "├── ds2_coding.pdf\n",
    "├── __init__.py\n",
    "├── main.py\n",
    "├── Mikolov et al.pdf\n",
    "├── notebooks\n",
    "│   └── Word2Vec training and evaluation.ipynb\n",
    "├── __pycache__\n",
    "│   └── main.cpython-311.pyc\n",
    "├── setup.py\n",
    "├── src\n",
    "│   ├── models.py\n",
    "│   ├── __pycache__\n",
    "│   └── utils.py\n",
    "```\n",
    "\n",
    "Root folder is \"**Word2Vec**\" which contains:\n",
    "\n",
    "- **Artifacts:** for saving processed data, models, plots and other artifacts.\n",
    "- **Data:** containing raw data\n",
    "- **main.py:** main file, but its better to use **Word2Vec training and evaluation** file in the notebooks folder\n",
    "- **src:** contains **utils.py** and **models.py** files with all the source code.\n",
    "    - utils.py has two classes _DataIO_ for data read and write, _DataLoader_ for creating batches and negative samples.\n",
    "    - models.py has three classes _SGNS_ - the model _per se_, _Word2Vec_ - model training wrapper and _EvaluateSGNS_ - to evaluate models.\n",
    "\n",
    "### Objective\n",
    "\n",
    "- **Skip-gram:**\n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{\\text{maximize}} \\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\le j \\le c,j\\ne0} \\log p(w_{t+j} | w_t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w_O | w_I) = \\frac{exp({v'_{w_O}}^T v_{w_I})}{\\sum_{w=1}^{W} exp(v'_w v_{w_I})}\n",
    "$$\n",
    "\n",
    "Skip-gram uses softmax to compute probability of words using softmax function which is very inefficient. Negative sampling reduces this computation by updating the objective funtion. \n",
    "\n",
    "- **Skip-gram with negative sampling:**\n",
    "\n",
    "$$\n",
    "\\underset{\\theta}{\\text{maximize}} \\sum_{(w,c) \\in D} \\log \\sigma(v_c^Tv_w) + \\sum_{(w,r) \\in D'} \\log \\sigma(-v_r^Tv_w)\n",
    "$$\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "For model evaluation I am just looking for closest word and manually check if it does make sense. There other ways like comparing model scores with bert scores for similar and dissimilar words.\n",
    "\n",
    "**Sample output:**\n",
    "```\n",
    "{'When': ['doing.', 'bookshelf.', 'acorns', 'car,\"', 'stumble'],\n",
    " 'diamond.': ['are.\"', 'cobweb', 'side!\"', 'day.\"', 'day:'],\n",
    " 'disgusting!': ['word.', 'appreciated', 'Leave', 'Pinchy', 'hole!\"'],\n",
    " 'fireplace.': ['band.', 'brave', 'jewel', 'flown', 'teeth,'],\n",
    " 'fruits': ['tree,', '\"Thanks', 'paint.', 'shelf.', 'calling'],\n",
    " 'insect.': ['Pandy.', 'secrets.', 'recommended', 'problems.Once', 'high!\"'],\n",
    " 'know!': ['elephant,', 'shadow.', 'back!', 'rough.', 'muffin.'],\n",
    " 'lungs': ['snake!', 'needle', 'park.Once', 'white', '\"Yuck!\"'],\n",
    " 'notes': ['ones,', 'Ash', 'tired.Once', 'organized', 'pumpkins'],\n",
    " 'please!': ['part', 'mine!\"', 'knot.', 'farm.Once', 'garden,']}\n",
    "```\n",
    "\n",
    "The results above shows top 5 most similar words to a given random word. The model for this was trained for 10000 steps for a vocab of 10000 words and embedding dimention of 300. The results don't look that good, will need to play around a bit with the parameters to tune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56a3333d-e869-40e6-a7c7-7fbd7704101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pprint as pp\n",
    "import torch\n",
    "from src.utils import DataIO, DataLoader\n",
    "from src.models import Word2Vec, EvaluateSGNS\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "855f9840-2ae9-4841-8c64-fe9cfeb4e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import ROOTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8849597b-3fca-4f3e-9f17-5267127138ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = 'Artifacts/metadata'\n",
    "FNAMES = [\n",
    "    'processed_data.pkl',\n",
    "    'word_counts.pkl',\n",
    "    'word2index.pkl',\n",
    "    'index2word.pkl'\n",
    "]\n",
    "# All paths are relative to root folder\n",
    "DATAPATH = ['Data/ds2.txt'] # path of training data\n",
    "MODELPATH = os.path.join(ROOTDIR, 'Artifacts/model/word2vec-2023-10-08 07:54:58.851242.pt') # path of the model\n",
    "SOURCEFILE = ['Data/ds1.txt'] #path of test data\n",
    "W2I = 'Artifacts/metadata/word2index.pkl'# path of word to index map\n",
    "I2W = 'Artifacts/metadata/index2word.pkl' # path of index to word map\n",
    "VOCAB_SIZE = 30000\n",
    "EMBED_SIZE = 300\n",
    "EXP_CONST = 3/4\n",
    "BATCH_SIZE = 128\n",
    "WINDOW = 4\n",
    "NEG_SAMPLES = 15\n",
    "HISTORY = 100\n",
    "EPOCHS = 100\n",
    "LR = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71bbdcc0-ed96-4f4d-aa97-8ce100fd1054",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_io = DataIO(ROOTDIR, filepath=DATAPATH, vocab_size=VOCAB_SIZE)\n",
    "data_io.process_data(filepath=FILEPATH, fnames=FNAMES)\n",
    "\n",
    "metadata = DataIO.load_data(root_dir=ROOTDIR, filepath=FILEPATH, fnames=FNAMES)\n",
    "loader = DataLoader(*metadata, exp_const=EXP_CONST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63e7cd1d-9a8b-4c19-9689-ccf474d9fc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8e922-c10f-4fd4-bd33-4d3668af7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at step no. 20800 of 100000 steps is 0.00031677945517003534, job is 20.8% complete\r"
     ]
    }
   ],
   "source": [
    "word2vec = Word2Vec(\n",
    "    root_dir=ROOTDIR,\n",
    "    train=True,\n",
    "    sgd=False,\n",
    "    process_data=False,\n",
    "    source_filepath=[],\n",
    "    metadata_filepath=FILEPATH,\n",
    "    metadata_fnames=FNAMES,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBED_SIZE,\n",
    "    learning_rate= LR\n",
    ")\n",
    "\n",
    "loss, modelname = word2vec.train(epochs=EPOCHS, steps=1000, window=WINDOW, k=NEG_SAMPLES, batch_size=BATCH_SIZE, loss_history=HISTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9047f-b1a1-4aa8-b723-643892717733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Word2Vec.load_sgns(vocab_size=VOCAB_SIZE, embedding_dim=EMBED_SIZE, path=MODELPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc78a6f-1aed-4df2-a9ad-4bd05572aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = EvaluateSGNS(\n",
    "    model=word2vec.model,\n",
    "    root_dir=ROOTDIR,\n",
    "    source_filepath=SOURCEFILE,\n",
    "    w2i_path=W2I,\n",
    "    i2w_path=I2W\n",
    ")\n",
    "output = evaluator.evaluate(ksamples=10, top_k=5)\n",
    "pp.pprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9012178f-1dcf-4816-8221-002eede7cc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "dlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
